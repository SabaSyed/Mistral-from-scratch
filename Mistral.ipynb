{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import dataclasses\n",
        "from typing import Optional, List"
      ],
      "metadata": {
        "id": "gSR04zEn5TMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class MoeArgs:  # MoE in mistral\n",
        "    n_experts: int\n",
        "    n_experts_per_tok: int\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 128      # embedding dimension for each input token and general dim in model layers\n",
        "    n_layers: int = 4   # number of transformer layers in the model\n",
        "    hidden_dim: int = 256   # hidden dimension used in ffn\n",
        "    head_dim: int = 32  # head dimension used in attention (conventionally set to hidden_dim / n_heads)\n",
        "    n_heads: int = 8  # number of heads for the Q\n",
        "    n_kv_heads: Optional[int] = None  # number of heads for the K and V (can be different from Q)\n",
        "    vocab_size: int = 1000  # vocab size (number of possible tokens) usually from tokenizer.vocab_size\n",
        "    norm_eps: float = 1e-5   # for numerical stability\n",
        "    max_batch_size: int = 8     # maximum batch size\n",
        "    max_seq_len: int = 64   # maximum sequence length (not directly used in Mistral)\n",
        "    attn_window: Optional[int] = None  # attention window and rolling buffer size, if None, it is set to max_seq_len\n",
        "    rope_theta: float = 10000.0  # theta for rotary embeddings\n",
        "    moe: Optional[MoeArgs] = None   # if set then use MoE otherwise normal FFN\n",
        "    debug: Optional[bool] = False   # if verbose\n",
        "    device: str = \"cpu\"  # device to use\n"
      ],
      "metadata": {
        "id": "VpN1WOUD5Tg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm"
      ],
      "metadata": {
        "id": "2ZJPev9R5af4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    # https://arxiv.org/pdf/1910.07467\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # The gamma (g) parameter that is trainable to perform the rescaling on the norm\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "        # RMSNorm statistics, (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
        "        rms_reciprocal = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)    # rsqrt = 1 / sqrt\n",
        "        return x * rms_reciprocal\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # This completes the equation 4 in the paper\n",
        "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
        "        # auto-broadcasting expands (Dim) to (1, 1, Dim) to multiplied to the last dimension of (B, Seq_Len, Dim)\n",
        "        # recall: Automatic broadcasting in PyTorch occurs when dimensions match or are broadcastable starting from the trailing dimensions (i.e., from right to left)\n",
        "        return self.weight * self._norm(x.float()).type_as(x)"
      ],
      "metadata": {
        "id": "mw-ADC7r5Wkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention with KV Cache\n",
        "\n",
        "**TODO**:\n",
        " * Create RollingBuffer class support\n",
        " * Add RollingBuffer in the self attention class"
      ],
      "metadata": {
        "id": "KQo5rwbe5oe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RollingBufferKVCache:\n",
        "    def __init__(self, max_batch_size, attn_window, n_kv_heads, head_dim):\n",
        "        # implemented based on idea from original Mistral paper https://arxiv.org/abs/2310.06825\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.attn_window = attn_window\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.head_dim = head_dim\n",
        "        # initialize the KV cache with zeros with shape (B, attn_window, n_kv_heads, Head_Dim)\n",
        "        self.cache_k = torch.zeros((self.max_batch_size, self.attn_window, self.n_kv_heads, self.head_dim))\n",
        "        self.cache_v = torch.zeros((self.max_batch_size, self.attn_window, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "    def update_cache(self, xk, xv, batch_size, start_pos):\n",
        "        # get the position of rolling window cache using the modulo operation\n",
        "        # ensures that the position wraps around within the attn_window size\n",
        "        cache_position = start_pos % self.attn_window\n",
        "        # update the entry in the KV cache's respective calculated position with the new KV values\n",
        "        # fill (:B, idx) part of the (max_B, max_seq_len, n_kv_heads, Head_Dim) cache with (B, 1, n_kv_heads, Head_Dim)\n",
        "        # shape of xk and xv: (batch_size, 1, n_kv_heads, head_dim)\n",
        "        self.cache_k[:batch_size, cache_position:cache_position + 1] = xk\n",
        "        self.cache_v[:batch_size, cache_position:cache_position + 1] = xv\n",
        "\n",
        "    def update_cache_multiple(self, xk, xv, batch_size, start_pos, seq_len):\n",
        "        # used when seq_len > 1, yet in inference we only care about the seq_len = 1 case\n",
        "        # can be optimized in the future to support Mistral's pre-fill and chunking (to handle prompts)\n",
        "        for i in range(seq_len):\n",
        "            self.update_cache(xk[:, i:i+1, :, :], xv[:, i:i+1, :, :], batch_size, start_pos + i)\n",
        "\n",
        "    def retrieve_cache(self, batch_size, start_pos):\n",
        "        # calculate the effective start position considering the rolling buffer's nature\n",
        "        # NOTE: start_pos should be updated to be start_pos + seq_len when called after update_cache\n",
        "        effective_start_pos = start_pos % self.attn_window\n",
        "        # retrieve KV from the cache, split into 2 parts to handle the wrap-around\n",
        "        keys = torch.cat([\n",
        "            self.cache_k[:batch_size, effective_start_pos:, :, :],\n",
        "            self.cache_k[:batch_size, :effective_start_pos, :, :]\n",
        "        ], dim=1)\n",
        "        values = torch.cat([\n",
        "            self.cache_v[:batch_size, effective_start_pos:, :, :],\n",
        "            self.cache_v[:batch_size, :effective_start_pos, :, :]\n",
        "        ], dim=1)\n",
        "        # select the last seq_len tokens from the concatenated keys and values (to handle when < attn_window)\n",
        "        keys = keys[:, -start_pos:, :, :]\n",
        "        values = values[:, -start_pos:, :, :]\n",
        "        return keys, values"
      ],
      "metadata": {
        "id": "7DBuJflG6QHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    # Decoder only with causal attention (only work for inference)\n",
        "    # only care about current token and its corresponding attention (with support from the KV Cache)\n",
        "    # Extended support for GQA (grouped query attention)\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.debug = args.debug\n",
        "        # set the number of KV heads for GQA (see the paper), default to Q heads (then just MHA)\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        # set the number of Q heads, should always be args.n_heads\n",
        "        self.n_heads_q = args.n_heads\n",
        "        # get num times the KV should be repeated in GQA\n",
        "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "        # dim of each head = dim / n_heads (the part of the embedding that each head will be responsible for)\n",
        "        self.head_dim = args.head_dim\n",
        "        # cache size or attention window size, if not specified, default to full attention\n",
        "        self.attn_window = args.attn_window if args.attn_window is not None else args.max_seq_len\n",
        "\n",
        "        # q k v o weights in transformer attention\n",
        "        self.wq = nn.Linear(args.dim, self.n_heads_q * self.head_dim)   # for Q\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim)  # for K\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim)  # for V\n",
        "        self.wo = nn.Linear(self.n_heads_q * self.head_dim, args.dim)   # for O (here n_heads_q * head_dim == dim)\n",
        "\n",
        "        # KV Cache with support of Sliding Window Attention & Rolling Buffer Cache\n",
        "        self.kv_cache = RollingBufferKVCache(\n",
        "            max_batch_size=args.max_batch_size,\n",
        "            attn_window=self.attn_window,\n",
        "            n_kv_heads=self.n_kv_heads,\n",
        "            head_dim=self.head_dim\n",
        "        )\n",
        "\n",
        "    def repeat_kv(self, kv: torch.Tensor) -> torch.Tensor:  # just copy, but can be optimized...\n",
        "        # in GQA, each Q group shares the same KV heads, thus just repeat KV heads for the Q in the same group\n",
        "        # goal shape: (B, prefix_seq_len, n_kv_heads, Head_Dim) => (B, prefix_seq_len, n_heads_q, Head_Dim)\n",
        "        batch_size, seq_len, n_kv_heads, head_dim = kv.shape\n",
        "        if self.n_rep == 1:  # Q and KV are 1-to-1 (just a normal MHA)\n",
        "            return kv\n",
        "        else:  # GQA\n",
        "            return (\n",
        "                # (B, prefix_seq_len, n_kv_heads, 1, Head_Dim)\n",
        "                kv[:, :, :, None, :]\n",
        "                # (B, prefix_seq_len, n_kv_heads, n_rep, Head_Dim) just copy n_rep times\n",
        "                .expand(batch_size, seq_len, n_kv_heads, self.n_rep, head_dim)\n",
        "                # (B, prefix_seq_len, n_kv_heads * n_rep, Head_Dim) = (B, prefix_seq_len, n_heads_q, Head_Dim)\n",
        "                .reshape(batch_size, seq_len, n_kv_heads * self.n_rep, head_dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor) -> torch.Tensor:\n",
        "        # recall start_pos = the position of the token in the sequence we are dealing with\n",
        "        # this is the standard self-attention mechanism computation with slight modifications (Llama / Mistral)\n",
        "        # goal shape: (B, 1, Dim) => (B, 1, Dim)\n",
        "        if self.debug: print(\"SelfAttention input shape\", x.shape)\n",
        "        batch_size, seq_len, _ = x.shape    # (B, 1, Dim)\n",
        "        assert seq_len == 1, \"only support 1D input for now for debugging\"  # TODO test support when seq_len > 1\n",
        "        # compute Q K V from the weights wq wk wv\n",
        "        # (B, 1, Dim) => (B, 1, n_heads_q * Head_Dim)\n",
        "        xq = self.wq(x)\n",
        "        # (B, 1, Dim) => (B, 1, n_kv_heads * Head_Dim)\n",
        "        xk = self.wk(x)\n",
        "        # (B, 1, Dim) => (B, 1, n_kv_heads * Head_Dim)\n",
        "        xv = self.wv(x)\n",
        "\n",
        "        # reshape Q K V to get individual single heads (Qi, Ki, Vi) from the tensors\n",
        "        # (B, 1, n_heads_q * Head_Dim) => (B, 1, n_heads_q, Head_Dim)\n",
        "        xq = xq.reshape(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "        # (B, 1, n_kv_heads * Head_Dim) => (B, 1, n_kv_heads, Head_Dim)\n",
        "        xk = xk.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        # (B, 1, n_kv_heads * Head_Dim) => (B, 1, n_kv_heads, Head_Dim)\n",
        "        xv = xv.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        # apply RoPE on Q and K, both should have the same shape before and after RoPE\n",
        "        xq = self.apply_rotary_embeddings(xq, freqs_complex, x.device)    # (B, 1, n_heads_q, Head_Dim)\n",
        "        xk = self.apply_rotary_embeddings(xk, freqs_complex, x.device)    # (B, 1, n_kv_heads, Head_Dim)\n",
        "\n",
        "        # replace the entry in the KV cache's respective position (aka update KV Cache)\n",
        "        # fill (:B, idx) part of the (max_B, max_seq_len, n_kv_heads, Head_Dim) cache with (B, 1, n_kv_heads, Head_Dim)\n",
        "        self.kv_cache.update_cache(xk, xv, batch_size, start_pos)\n",
        "\n",
        "        # retrieve complete K and V from KV Cache for Attention Computation\n",
        "        # (B, prefix_seq_len, n_kv_heads, Head_Dim)\n",
        "        keys, values = self.kv_cache.retrieve_cache(batch_size, start_pos + seq_len)\n",
        "\n",
        "        # in GQA, each Q group shares the same KV heads, thus just repeat KV heads for the Q in the same group\n",
        "        # (B, prefix_seq_len, n_kv_heads, Head_Dim) => (B, prefix_seq_len, n_heads_q, Head_Dim)\n",
        "        keys = self.repeat_kv(keys)\n",
        "        values = self.repeat_kv(values)\n",
        "\n",
        "        # reshape: equivalent to X.reshape(B, n_heads_q, 1 or prefix_seq_len, Head_Dim)\n",
        "        # (B, 1, n_heads_q, Head_Dim) => (B, n_heads_q, 1, Head_Dim)\n",
        "        xq = xq.transpose(1, 2)\n",
        "        # (B, prefix_seq_len, n_heads_q, Head_Dim) => (B, n_heads_q, prefix_seq_len, Head_Dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # attention weight and score computation\n",
        "        # NOTE about MATMUL: for tensors with more than 2 dimensions, torch.matmul treats the last two dimensions as matrices and performs batch matrix multiplication on the other dimensions. The result is a tensor where each batch element is the result of matrix multiplication on the corresponding batch elements of the input tensors\n",
        "        # (B, n_heads_q, 1, Head_Dim) @ (B, n_heads_q, Head_Dim, prefix_seq_len) => (B, n_heads_q, 1, prefix_seq_len)\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        # softmax(QK/sqrt(dk)): (B, n_heads_q, 1, prefix_seq_len) => (B, n_heads_q, 1, prefix_seq_len)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)  # dim=-1 means softmax along last dimension (sum=1)\n",
        "\n",
        "        # attention computation with the values\n",
        "        # (B, n_heads_q, 1, prefix_seq_len) @ (B, n_heads_q, prefix_seq_len, Head_Dim) => (B, n_heads_q, 1, Head_Dim)\n",
        "        output = torch.matmul(scores, values)\n",
        "        # (B, n_heads_q, 1, Head_Dim) => (B, 1, n_heads_q, Head_Dim) and make sure contiguous in memory\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        # (B, 1, n_heads_q, Head_Dim) => (B, 1, n_heads_q * Head_Dim) = (B, 1, Dim)\n",
        "        output = output.reshape(batch_size, seq_len, self.n_heads_q * self.head_dim)\n",
        "\n",
        "        # final linear layer\n",
        "        # (B, 1, Dim) => (B, 1, Dim)\n",
        "        output = self.wo(output)\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"SelfAttention output shape\", output.shape)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotary_embeddings(x: torch.Tensor, freqs_cis: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
        "        # reshape x: (B, 1, n_heads_q, Head_Dim) => (B, 1, n_heads_q, 2, Head_Dim/2)\n",
        "        x = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "        x = torch.view_as_complex(x)\n",
        "        # apply the RoPE rotation\n",
        "        x = x * freqs_cis.to(device)\n",
        "        # reshape back x: (B, 1, n_heads_q, 2, Head_Dim/2) => (B, 1, n_heads_q, Head_Dim)\n",
        "        x = torch.view_as_real(x)\n",
        "        return x.flatten(3)\n"
      ],
      "metadata": {
        "id": "84hGmZYh5cqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedforward"
      ],
      "metadata": {
        "id": "r_Qxkyfc8f8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    # Initialize the FeedForward class\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()  # Call the constructor of the parent class (nn.Module)\n",
        "        self.debug = args.debug  # Store the debug flag from the arguments\n",
        "        hidden_dim = args.hidden_dim  # Get the hidden dimension size from the arguments\n",
        "\n",
        "        # Initialize the linear layers (weights) of the FFN\n",
        "        # First linear layer: transforms input from 'dim' to 'hidden_dim'\n",
        "        # Second linear layer: transforms back from 'hidden_dim' to 'dim'\n",
        "        # Third linear layer: another transformation from 'dim' to 'hidden_dim'\n",
        "        self.w1 = nn.Linear(args.dim, hidden_dim)  # Linear layer for w1\n",
        "        self.w2 = nn.Linear(hidden_dim, args.dim)  # Linear layer for w2\n",
        "        self.w3 = nn.Linear(args.dim, hidden_dim)  # Linear layer for w3\n",
        "\n",
        "    # Define the forward pass of the network\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.debug:  # If debugging is enabled, print the shape of the input tensor\n",
        "            print(\"FeedForward input shape\", x.shape)\n",
        "\n",
        "        # Apply the first linear transformation (w1) to the input\n",
        "        # This maps the input from the original dimension (Dim) to the hidden dimension (Hidden_Dim)\n",
        "        x_w1 = self.w1(x)  # (B, Seq, Hidden_Dim)\n",
        "\n",
        "        # Apply the SiLU activation function to the output of the first linear transformation\n",
        "        # This activation function adds non-linearity to the network\n",
        "        x_silu = F.silu(x_w1)  # (B, Seq, Hidden_Dim)\n",
        "\n",
        "        # Apply the third linear transformation (w3) to the input\n",
        "        # This again maps the input from the original dimension (Dim) to the hidden dimension (Hidden_Dim)\n",
        "        x_w3 = self.w3(x)  # (B, Seq, Hidden_Dim)\n",
        "\n",
        "        # Perform element-wise multiplication between the activated output (x_silu) and the result of the third linear transformation (x_w3)\n",
        "        # This is part of the gating mechanism in SwiGLU, where one part controls the other\n",
        "        x_swiglu = x_silu * x_w3  # (B, Seq, Hidden_Dim)\n",
        "\n",
        "        # Apply the second linear transformation (w2) to the result of the element-wise multiplication\n",
        "        # This maps the output back from the hidden dimension (Hidden_Dim) to the original input dimension (Dim)\n",
        "        output = self.w2(x_swiglu)  # (B, Seq, Dim)\n",
        "\n",
        "        if self.debug:  # If debugging is enabled, print the shape of the output tensor\n",
        "            print(\"FeedForward output shape\", output.shape)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "r6xp2et58iQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOE"
      ],
      "metadata": {
        "id": "4M4xU6NaAGlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "\n",
        "class MoE(nn.Module):\n",
        "    # Initialize the MoE layer with a list of experts, a gate module, and MoE-specific arguments\n",
        "    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoeArgs):\n",
        "        super().__init__()\n",
        "        # Ensure the number of provided experts matches the expected number from moe_args\n",
        "        assert len(experts) == moe_args.n_experts, \"Number of experts must be equal to n_experts\"\n",
        "\n",
        "        # Store the list of experts (each expert is a separate feed-forward network or another nn.Module)\n",
        "        self.experts = nn.ModuleList(experts)\n",
        "\n",
        "        # The gating mechanism: a linear layer that determines which experts to use for each input\n",
        "        self.gate = gate\n",
        "\n",
        "        # Store additional MoE-specific arguments\n",
        "        self.moe_args = moe_args\n",
        "\n",
        "    # Forward pass through the MoE layer\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # NOTE: In the Mistral paper, the input/output sizes are reshaped to (B * seq_len, Dim) instead of (B, seq_len, Dim)\n",
        "        # Goal shape transformation: (B, seq_len, Dim) => (B, seq_len, Dim)\n",
        "\n",
        "        # Get the batch size (B), sequence length (seq_len), and dimension (dim) from the input tensor\n",
        "        B, seq_len, dim = x.shape\n",
        "\n",
        "        # Flatten the input to match the expected shape for the gating mechanism: (B * seq_len, Dim)\n",
        "        x_flat = x.view(-1, dim)  # (B * seq_len, Dim)\n",
        "\n",
        "        # Pass the flattened input through the gate to get logits for each expert\n",
        "        # (B * seq_len, Dim) gate=> (B * seq_len, n_experts)\n",
        "        # The gate produces a score (logit) for each expert\n",
        "        logits = self.gate(x_flat)  # (B * seq_len, n_experts)\n",
        "\n",
        "        # Select the top-k experts based on the gate logits for each input token\n",
        "        # `torch.topk` returns the highest `n_experts_per_tok` logits and their corresponding expert indices\n",
        "        # weights=logits, selected_experts=indices\n",
        "        # (B * seq_len, n_experts) => (B * seq_len, n_experts_per_tok)\n",
        "        topk_logits, topk_indices = torch.topk(logits, self.moe_args.n_experts_per_tok, dim=1)  # top-k experts\n",
        "\n",
        "        # Normalize the selected expert weights using softmax, so that they sum to 1 for each input token\n",
        "        topk_weights = torch.softmax(topk_logits, dim=1)  # (B * seq_len, n_experts_per_tok)\n",
        "\n",
        "        # Initialize the output tensor to store the final results: (B * seq_len, Dim)\n",
        "        results = torch.zeros_like(x_flat)  # (B * seq_len, Dim)\n",
        "\n",
        "        # Iterate over each expert to compute the weighted sum of the outputs from each selected top-k expert\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Determine which tokens are assigned to the current expert\n",
        "            # `torch.where` returns indices where the selected_experts match the current expert index `i`\n",
        "            expert_mask = topk_indices == i  # (B * seq_len, n_experts_per_tok)\n",
        "\n",
        "            if expert_mask.any():  # Check if any tokens are assigned to this expert\n",
        "                # Extract the input tokens assigned to this expert: (K, Dim), where K is the number of tokens assigned\n",
        "                expert_input = x_flat[expert_mask.any(dim=1)]  # (K, Dim)\n",
        "\n",
        "                # Pass the selected tokens through the expert\n",
        "                expert_output = expert(expert_input)  # (K, Dim)\n",
        "\n",
        "                # Get the corresponding weights for the selected tokens: (K, 1)\n",
        "                expert_weights = topk_weights[expert_mask]  # (K,)\n",
        "\n",
        "                # Multiply the expert output by its weight and add the result to the final output tensor\n",
        "                # expert_w * expert_out: (K,) * (K, Dim) => (K, Dim)\n",
        "                weighted_output = expert_output * expert_weights.unsqueeze(1)  # (K, Dim)\n",
        "                results[expert_mask.any(dim=1)] += weighted_output  # Update the results\n",
        "\n",
        "        # Reshape the results back to the original input shape: (B * seq_len, Dim) => (B, seq_len, Dim)\n",
        "        results = results.view(B, seq_len, dim)  # (B, seq_len, Dim)\n",
        "\n",
        "        # Return the final output of the MoE layer\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "XQ4tWsQPAJ63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block"
      ],
      "metadata": {
        "id": "l42-1KbV_Eh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "# The TransformerBlock class\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.debug = args.debug\n",
        "        self.n_head = args.n_heads\n",
        "        self.dim = args.dim\n",
        "\n",
        "        # RMSNorm layers for attention and feed-forward\n",
        "        self.rms_norm_attn = nn.LayerNorm(self.dim, eps=args.norm_eps)\n",
        "        self.rms_norm_ffn = nn.LayerNorm(self.dim, eps=args.norm_eps)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.self_attention = nn.MultiheadAttention(self.dim, self.n_head, batch_first=True)\n",
        "\n",
        "        # Define the experts (example)\n",
        "        self.experts = [nn.Sequential(nn.Linear(args.dim, args.hidden_dim), nn.ReLU(), nn.Linear(args.hidden_dim, args.dim)) for _ in range(args.moe.n_experts)]\n",
        "\n",
        "        # Define the gate (example)\n",
        "        self.gate = nn.Linear(args.dim, args.moe.n_experts)\n",
        "\n",
        "        # MoE or standard feed-forward layer\n",
        "        if args.moe:\n",
        "            # Assuming args.moe contains experts, gate, and moe_args\n",
        "            self.feed_forward = MoE(self.experts, self.gate, args.moe)\n",
        "        else:\n",
        "            self.feed_forward = FeedForward(args)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor) -> torch.Tensor:\n",
        "        if self.debug:\n",
        "            print(\"TransformerBlock input shape:\", x.shape)\n",
        "\n",
        "        # Apply RMSNorm and self-attention, then add the result to the original input (residual connection)\n",
        "        x = self.rms_norm_attn(x)\n",
        "        attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Apply RMSNorm and the feed-forward network, then add the result to the previous output (residual connection)\n",
        "        x = self.rms_norm_ffn(x)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        out = x + ff_output\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "w2euT_7o_Gtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "WR577f4M_Ht2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        assert args.vocab_size > 0, \"vocab size should be set\"  # Ensure vocab size is provided\n",
        "        self.args = args\n",
        "        self.debug = args.debug  # Debugging flag\n",
        "        self.vocab_size = args.vocab_size  # Vocabulary size for token embeddings\n",
        "        self.n_layers = args.n_layers  # Number of transformer layers\n",
        "\n",
        "        # Token embedding layer: maps input tokens to their vector representations\n",
        "        self.embedding = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "        # Transformer layers: Stacking multiple TransformerBlocks to form the deep model\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerBlock(args) for _ in range(self.n_layers)\n",
        "        ])\n",
        "\n",
        "        # RMS Normalization after all layers\n",
        "        self.rms_norm = nn.LayerNorm(args.dim)  # RMSNorm can be used instead of LayerNorm\n",
        "\n",
        "        # Output layer: Projects the final hidden states to the vocabulary size for prediction\n",
        "        self.output = nn.Linear(args.dim, self.vocab_size)\n",
        "\n",
        "        # Precompute frequencies for ROPE positional encoding as described in the paper (for more efficient processing)\n",
        "        self.freqs_complex = self.precompute_theta_pos_frequencies(\n",
        "            head_dim=args.dim * 2,  # Assuming head_dim is dim\n",
        "            seq_len=args.max_seq_len,  # Set max_seq_len or adjust based on your use case\n",
        "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Assuming device is set to 'cuda' or 'cpu'\n",
        "            theta=args.rope_theta  # Preset theta value\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def dtype(self) -> torch.dtype:\n",
        "        # Returns the data type of the model parameters\n",
        "        return next(self.parameters()).dtype\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        # Returns the device (CPU/GPU) on which the model parameters are stored\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    # Forward pass through the entire transformer model\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int) -> torch.Tensor:\n",
        "\n",
        "      if self.debug:\n",
        "          print(\"Transformer input shape\", tokens.shape)\n",
        "\n",
        "      batch_size, seq_len = tokens.shape\n",
        "      assert seq_len == 1, \"One token at a time at inference time\"\n",
        "\n",
        "      embeddings = self.embedding(tokens)  # Shape: (batch_size, seq_len, dim)\n",
        "\n",
        "      print(\"Start Position \", start_pos, \"Seq Length\", seq_len, \"batch Size\", batch_size)\n",
        "\n",
        "      # Get the positional encodings\n",
        "      pos_encodings = self.freqs_complex[start_pos:start_pos + seq_len]  # (seq_len, dim)\n",
        "      pos_encodings = pos_encodings.expand(batch_size, seq_len, -1)  # (batch_size, seq_len, dim)\n",
        "      pos_encodings = pos_encodings.to(torch.float32)  # Ensure pos_encodings are float32\n",
        "\n",
        "      print(\"Embeddings shape:\", embeddings.shape)\n",
        "      print(\"Positional encodings shape:\", pos_encodings.shape)\n",
        "\n",
        "      # Add positional encodings to token embeddings\n",
        "      embeddings = embeddings + pos_encodings\n",
        "\n",
        "      # Pass the embeddings through each transformer layer sequentially\n",
        "      h = embeddings\n",
        "      for layer in self.transformer_layers:\n",
        "          h = layer(h, start_pos, self.freqs_complex)\n",
        "\n",
        "      h = self.rms_norm(h)\n",
        "      output = self.output(h)\n",
        "      return output\n",
        "\n",
        "\n",
        "\n",
        "    # Precompute the positional encoding frequencies for ROPE\n",
        "    def precompute_theta_pos_frequencies(\n",
        "         self, head_dim: int, seq_len: int, device: torch.device, theta: float\n",
        "         ) -> torch.Tensor:\n",
        "\n",
        "      \"\"\"\n",
        "      Precomputes the frequencies used in ROPE positional encoding.\n",
        "      \"\"\"\n",
        "      assert head_dim % 2 == 0, \"head_dim must be even\"\n",
        "\n",
        "      # Calculate the sequence of theta values\n",
        "      theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "      theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
        "\n",
        "      # Generate the positions 'm' as a sequence from 0 to seq_len-1\n",
        "      m = torch.arange(seq_len, device=device)\n",
        "\n",
        "      # Compute the outer product of positions 'm' and theta values\n",
        "      freqs = torch.outer(m, theta).float()\n",
        "\n",
        "      # Compute the polar form (complex number) used in ROPE\n",
        "      freqs_complex = torch.polar(torch.ones_like(freqs, dtype=torch.float32), freqs)\n",
        "      print(\"freqs_complex shape:\", freqs_complex.shape)\n",
        "      return freqs_complex\n",
        "\n"
      ],
      "metadata": {
        "id": "h8Us3LCl_Jcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral"
      ],
      "metadata": {
        "id": "CxcHo2LhAmYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Tuple\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Mistral:\n",
        "    def __init__(self, model_args: ModelArgs):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.args = model_args\n",
        "        self.model = Transformer(args=model_args).to(device)\n",
        "\n",
        "    def generate(\n",
        "            self, prompts: List[List[int]], temperature: float = 0.6,\n",
        "            top_p: float = 0.9, max_gen_len: Optional[int] = None\n",
        "    ):\n",
        "\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.args.max_seq_len - 1\n",
        "\n",
        "        prompt_tokens = prompts\n",
        "        batch_size = len(prompt_tokens)\n",
        "        assert batch_size <= self.args.max_batch_size, \\\n",
        "            f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
        "\n",
        "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "        assert max_prompt_len <= self.args.max_seq_len, \\\n",
        "            f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
        "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "        pad_id = 0\n",
        "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=self.model.device)\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=self.model.device)\n",
        "\n",
        "        eos_reached = torch.tensor([False] * batch_size, device=self.model.device)\n",
        "        prompt_tokens_mask = tokens != pad_id\n",
        "\n",
        "        for cur_pos in tqdm(range(1, total_len), desc=\"Generating tokens\"):\n",
        "            with torch.no_grad():\n",
        "                logits = self.model.forward(tokens[:, cur_pos - 1:cur_pos], cur_pos)\n",
        "\n",
        "            next_token = self.sample_next_token(logits, temperature, top_p)\n",
        "            next_token = next_token.reshape(-1)\n",
        "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == -1)\n",
        "            if all(eos_reached): break\n",
        "\n",
        "        out_tokens = []\n",
        "        out_text = []\n",
        "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "            if -1 in current_prompt_tokens:\n",
        "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
        "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "            out_tokens.append(current_prompt_tokens)\n",
        "            # out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "        return (out_tokens, out_text)\n",
        "\n",
        "    def sample_next_token(self, probs, temperature, top_p):\n",
        "        if temperature == 0:\n",
        "            next_token = torch.argmax(probs[:, -1], dim=-1)\n",
        "            return next_token\n",
        "\n",
        "        probs = torch.softmax(probs[:, -1] / temperature, dim=-1)\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "        mask = probs_sum - probs_sort > top_p\n",
        "        probs_sort[mask] = 0.0\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "        next_token = torch.gather(probs_idx, -1, next_token)\n",
        "        return next_token\n"
      ],
      "metadata": {
        "id": "ceYJAfr1An58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "id": "ftufErrBAtbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa50cbff-0fb6-4e66-dbc4-bf0bc4eaed53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=8259b683a3c65c1084e9de755c4d0232376abff8b65f4dca167c878afa3e2696\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchviz"
      ],
      "metadata": {
        "id": "awk7kAXKAv1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b322856-75c7-4c28-d3c3-95434062193e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchviz)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchviz)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchviz)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchviz)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchviz)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchviz)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=a37c4656d1f5ec1e99efc3c59a28b9042f9198c1c49453d52d889bcf27756758\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Ensure all necessary classes and functions are imported\n",
        "# from mistral_model import Transformer, TransformerBlock, ModelArgs, MoeArgs, FeedForward\n",
        "# from mistral_inference import Mistral\n",
        "\n",
        "# Define necessary model arguments\n",
        "args = ModelArgs(\n",
        "    dim=128,\n",
        "    n_layers=1,\n",
        "    hidden_dim=256,\n",
        "    head_dim=16,\n",
        "    n_heads=8,\n",
        "    n_kv_heads=2,\n",
        "    vocab_size=1000,\n",
        "    norm_eps=1e-5,\n",
        "    max_batch_size=8,\n",
        "    max_seq_len=64,\n",
        "    attn_window=4,\n",
        "    rope_theta=10000.0,\n",
        "    moe=MoeArgs(n_experts=4, n_experts_per_tok=2),\n",
        "    debug=False\n",
        ")\n",
        "\n",
        "\n",
        "device =  \"cpu\"\n",
        "mistral = Mistral(args)\n",
        "print(mistral.model)\n",
        "\n",
        "# Visualize the model\n",
        "model = Transformer(args).to(device)\n",
        "x = torch.ones(3, 1, dtype=torch.long, device=device)\n",
        "y = model(x, 0)\n",
        "\n",
        "params = {name: param for name, param in model.named_parameters() if 'bias' not in name}\n",
        "dot = make_dot(y, params=params)\n",
        "\n",
        "# Simplify node names\n",
        "simple_names = {}\n",
        "for node in dot.body:\n",
        "    if 'Conv' in node or 'BatchNorm' in node or 'ReLU' in node:\n",
        "        name = node.split('[')[0]\n",
        "        simple_names[name] = name.replace('\\\"', '').split('/')[-1]\n",
        "\n",
        "for i in range(len(dot.body)):\n",
        "    for key, value in simple_names.items():\n",
        "        if key in dot.body[i]:\n",
        "            dot.body[i] = dot.body[i].replace(key, value)\n",
        "\n",
        "dot.render('model_visualization', format='png')\n",
        "\n",
        "# Generate tokens\n",
        "encoded_prompts = [[10, 2, 4, 4, 3, 7, 8], [4, 5, 6, 2, 3, 8, 9], [4, 5, 6, 2, 3, 4, 5, 6, 2, 3]]\n",
        "tokens, text = mistral.generate(\n",
        "    prompts=encoded_prompts,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "    max_gen_len=10\n",
        ")\n",
        "print(\"Generated tokens:\", tokens)\n",
        "\n",
        "# Instantiate transformer and check parameters\n",
        "transformer_block = TransformerBlock(args=args).to(device)\n",
        "transformer = Transformer(args=args).to(device)\n",
        "tok = transformer.embedding\n",
        "vocab_size = 1000\n",
        "batch_size = 3  # Example batch size\n",
        "encoded_prompts = [[100, 2, 4, 4, 3, 7, 8], [4, 5, 6, 2, 3, 8, 9], [4, 5, 6, 2, 3, 4, 5, 6, 2, 3]]\n",
        "\n",
        "print(encoded_prompts)\n",
        "prompt_chunks = [p[0:1] for p in encoded_prompts]\n",
        "input_tensor = torch.tensor(sum(prompt_chunks, []), device=device, dtype=torch.long)\n",
        "print(input_tensor.shape)\n",
        "print(tok(input_tensor).shape)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(transformer)\n",
        "print(f\"Total trainable parameters in the model: {total_params}\")\n",
        "\n",
        "# Inference without cache\n",
        "input_tensor = torch.tensor([[vocab_size - 1]] * batch_size, dtype=torch.long, device=device)\n",
        "start_pos = 0\n",
        "output = transformer(input_tensor, start_pos)\n",
        "print(\"Inference output:\", output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ro_pxcFZAy0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4bc3c3-abfa-4e54-8c84-2ac37e3b3943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freqs_complex shape: torch.Size([64, 128])\n",
            "Transformer(\n",
            "  (embedding): Embedding(1000, 128)\n",
            "  (transformer_layers): ModuleList(\n",
            "    (0): TransformerBlock(\n",
            "      (rms_norm_attn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (rms_norm_ffn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attention): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (gate): Linear(in_features=128, out_features=4, bias=True)\n",
            "      (feed_forward): MoE(\n",
            "        (experts): ModuleList(\n",
            "          (0-3): 4 x Sequential(\n",
            "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (gate): Linear(in_features=128, out_features=4, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (rms_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (output): Linear(in_features=128, out_features=1000, bias=True)\n",
            ")\n",
            "freqs_complex shape: torch.Size([64, 128])\n",
            "Start Position  0 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 19/19 [00:00<00:00, 116.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Position  1 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  2 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  3 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  4 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  5 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  6 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  7 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  8 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  9 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  10 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  11 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  12 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  13 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  14 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  15 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  16 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  17 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  18 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Start Position  19 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Generated tokens: [[10, 2, 4, 4, 3, 7, 8, 723, 396, 917, 52, 594, 252, 29, 684, 620, 915, 175, 309, 513], [4, 5, 6, 2, 3, 8, 9, 830, 551, 383, 171, 216, 455, 484, 976, 136, 26, 781, 487, 977], [4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 338, 880, 844, 948, 651, 106, 765, 964, 43, 83]]\n",
            "freqs_complex shape: torch.Size([64, 128])\n",
            "[[100, 2, 4, 4, 3, 7, 8], [4, 5, 6, 2, 3, 8, 9], [4, 5, 6, 2, 3, 4, 5, 6, 2, 3]]\n",
            "torch.Size([3])\n",
            "torch.Size([3, 128])\n",
            "Total trainable parameters in the model: 588012\n",
            "Start Position  0 Seq Length 1 batch Size 3\n",
            "Embeddings shape: torch.Size([3, 1, 128])\n",
            "Positional encodings shape: torch.Size([3, 1, 128])\n",
            "Inference output: tensor([[[ 0.6139, -0.7737,  0.2122,  ..., -0.5087, -0.6487,  0.0449]],\n",
            "\n",
            "        [[ 0.6139, -0.7737,  0.2122,  ..., -0.5087, -0.6487,  0.0449]],\n",
            "\n",
            "        [[ 0.6139, -0.7737,  0.2122,  ..., -0.5087, -0.6487,  0.0449]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}